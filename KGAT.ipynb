{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import scipy.sparse as sp\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging Setup\n",
    "def create_log_id(dir_path):\n",
    "    log_count = 0\n",
    "    while os.path.exists(os.path.join(dir_path, f'log{log_count}.log')):\n",
    "        log_count += 1\n",
    "    return log_count\n",
    "\n",
    "def logging_config(folder=None, name='log', level=logging.INFO, no_console=True):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    log_path = os.path.join(folder, name + \".log\")\n",
    "    print(f\"Logging to {log_path}\")\n",
    "    logging.basicConfig(level=level,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                        handlers=[\n",
    "                            logging.FileHandler(log_path),\n",
    "                            logging.StreamHandler() if not no_console else logging.NullHandler()\n",
    "                        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Functions (precision, recall, ndcg)\n",
    "def precision_at_k_batch(hits, k):\n",
    "    return hits[:, :k].mean(axis=1)\n",
    "\n",
    "def recall_at_k_batch(hits, k):\n",
    "    res = hits[:, :k].sum(axis=1) / hits.sum(axis=1)\n",
    "    return res\n",
    "\n",
    "def ndcg_at_k_batch(hits, k):\n",
    "    hits_k = hits[:, :k]\n",
    "    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]\n",
    "    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "    idcg[idcg == 0] = np.inf\n",
    "    return dcg / idcg\n",
    "\n",
    "def calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks):\n",
    "    test_binary = np.zeros((len(user_ids), len(item_ids)), dtype=np.float32)\n",
    "    for i, u in enumerate(user_ids):\n",
    "        test_items = test_user_dict[u]\n",
    "        train_items = train_user_dict[u]\n",
    "        cf_scores[i][train_items] = -np.inf\n",
    "        test_binary[i][test_items] = 1.0\n",
    "\n",
    "    try:\n",
    "        _, rank_indices = torch.sort(cf_scores.cuda(), descending=True)\n",
    "    except:\n",
    "        _, rank_indices = torch.sort(cf_scores, descending=True)\n",
    "    rank_indices = rank_indices.cpu()\n",
    "\n",
    "    binary_hit = np.array([test_binary[i][rank_indices[i]] for i in range(len(user_ids))], dtype=np.float32)\n",
    "\n",
    "    results = {}\n",
    "    for k in Ks:\n",
    "        results[k] = {\n",
    "            'precision': precision_at_k_batch(binary_hit, k),\n",
    "            'recall': recall_at_k_batch(binary_hit, k),\n",
    "            'ndcg': ndcg_at_k_batch(binary_hit, k)\n",
    "        }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "\n",
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1) / 2.)\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        if aggregator_type == 'gcn':\n",
    "            self.linear = nn.Linear(in_dim, out_dim)\n",
    "        elif aggregator_type == 'graphsage':\n",
    "            self.linear = nn.Linear(in_dim * 2, out_dim)\n",
    "        elif aggregator_type == 'bi-interaction':\n",
    "            self.linear1 = nn.Linear(in_dim, out_dim)\n",
    "            self.linear2 = nn.Linear(in_dim, out_dim)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            embeddings = self.activation(self.linear(ego_embeddings + side_embeddings))\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            embeddings = self.activation(self.linear(torch.cat([ego_embeddings, side_embeddings], dim=1)))\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            sum_embed = self.activation(self.linear1(ego_embeddings + side_embeddings))\n",
    "            bi_embed = self.activation(self.linear2(ego_embeddings * side_embeddings))\n",
    "            embeddings = sum_embed + bi_embed\n",
    "        return self.dropout(embeddings)\n",
    "\n",
    "class KGAT(nn.Module):\n",
    "    def __init__(self, args, n_users, n_entities, n_relations, A_in=None, user_pre_embed=None, item_pre_embed=None):\n",
    "        super().__init__()\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.relation_dim = args.relation_dim\n",
    "        self.n_users = n_users\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "\n",
    "        self.entity_user_embed = nn.Embedding(n_entities + n_users, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(n_relations, self.relation_dim)\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(n_relations, self.embed_dim, self.relation_dim))\n",
    "\n",
    "        if args.use_pretrain == 1 and user_pre_embed is not None:\n",
    "            entity_user_embed = torch.cat([item_pre_embed, nn.Parameter(torch.Tensor(n_entities - item_pre_embed.shape[0], self.embed_dim)), user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        self.aggregation_type = args.aggregation_type\n",
    "        self.conv_dim_list = [args.embed_dim] + eval(args.conv_dim_list)\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.n_layers = len(self.conv_dim_list) - 1\n",
    "\n",
    "        self.aggregator_layers = nn.ModuleList([\n",
    "            Aggregator(self.conv_dim_list[i], self.conv_dim_list[i+1], self.mess_dropout[i], self.aggregation_type)\n",
    "            for i in range(self.n_layers)\n",
    "        ])\n",
    "\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(n_users + n_entities, n_users + n_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in\n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "        for layer in self.aggregator_layers:\n",
    "            ego_embed = layer(ego_embed, self.A_in)\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "        return torch.cat(all_embed, dim=1)\n",
    "\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        all_embed = self.calc_cf_embeddings()\n",
    "        user_embed = all_embed[user_ids]\n",
    "        item_embed = all_embed[item_ids]\n",
    "        return torch.matmul(user_embed, item_embed.transpose(0, 1))\n",
    "\n",
    "    def forward(self, *input, mode):\n",
    "        if mode == 'predict':\n",
    "            return self.calc_score(*input)\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaderKGAT Definition\n",
    "class DataLoaderKGAT:\n",
    "    def __init__(self, args, logger):\n",
    "        self.logger = logger\n",
    "        self.data_path = os.path.join(args.data_dir, args.data_name)\n",
    "\n",
    "        self.train_user_dict = self._load_user_item_dict('train.txt')\n",
    "        self.test_user_dict = self._load_user_item_dict('test.txt')\n",
    "        self.n_users, self.n_items = self._get_user_item_num()\n",
    "\n",
    "        kg_file = os.path.join(self.data_path, 'kg_final.txt')\n",
    "        self.kg_data, self.n_entities, self.n_relations = self._load_kg(kg_file)\n",
    "        self.n_users_entities = self.n_users + self.n_entities\n",
    "\n",
    "        self.train_kg_dict = self._construct_kg_dict(self.kg_data)\n",
    "        self.h_list, self.t_list, self.r_list = self._build_relation_triplets()\n",
    "        self.A_in, self.laplacian_dict = self._build_sparse_graph()\n",
    "\n",
    "        self.cf_batch_size = args.cf_batch_size\n",
    "        self.kg_batch_size = args.kg_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "\n",
    "        self.n_cf_train = sum([len(v) for v in self.train_user_dict.values()])\n",
    "        self.n_kg_train = len(self.kg_data)\n",
    "\n",
    "        if args.use_pretrain == 1:\n",
    "            self.user_pre_embed = np.load(os.path.join(args.pretrain_embedding_dir, args.data_name, 'user_embed.npy'))\n",
    "            self.item_pre_embed = np.load(os.path.join(args.pretrain_embedding_dir, args.data_name, 'item_embed.npy'))\n",
    "\n",
    "    def _load_user_item_dict(self, filename):\n",
    "        user_dict = dict()\n",
    "        filepath = os.path.join(self.data_path, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            for line in f:\n",
    "                items = list(map(int, line.strip().split()))\n",
    "                if len(items) < 2:\n",
    "                    continue\n",
    "                user, item_list = items[0], items[1:]\n",
    "                user_dict[user] = item_list\n",
    "        return user_dict\n",
    "\n",
    "    def _get_user_item_num(self):\n",
    "        n_users = max(self.train_user_dict.keys()) + 1\n",
    "        all_items = set()\n",
    "        for item_list in self.train_user_dict.values():\n",
    "            all_items.update(item_list)\n",
    "        n_items = max(all_items) + 1\n",
    "        return n_users, n_items\n",
    "\n",
    "    def _load_kg(self, file_path):\n",
    "        kg_data = []\n",
    "        entities, relations = set(), set()\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                h, r, t = map(int, line.strip().split())\n",
    "                kg_data.append((h, r, t))\n",
    "                entities.update([h, t])\n",
    "                relations.add(r)\n",
    "        return kg_data, max(entities) + 1, max(relations) + 1\n",
    "\n",
    "    def _construct_kg_dict(self, kg_data):\n",
    "        kg_dict = dict()\n",
    "        for h, r, t in kg_data:\n",
    "            if h not in kg_dict:\n",
    "                kg_dict[h] = []\n",
    "            kg_dict[h].append((t, r))\n",
    "        return kg_dict\n",
    "\n",
    "    def _build_relation_triplets(self):\n",
    "        h_list, t_list, r_list = [], [], []\n",
    "        for h, pairs in self.train_kg_dict.items():\n",
    "            for t, r in pairs:\n",
    "                h_list.append(h)\n",
    "                t_list.append(t)\n",
    "                r_list.append(r)\n",
    "        return torch.LongTensor(h_list), torch.LongTensor(t_list), torch.LongTensor(r_list)\n",
    "\n",
    "    def _build_sparse_graph(self):\n",
    "        rows, cols = [], []\n",
    "        for user in self.train_user_dict:\n",
    "            for item in self.train_user_dict[user]:\n",
    "                rows.append(user)\n",
    "                cols.append(self.n_users + item)\n",
    "        for h, r, t in self.kg_data:\n",
    "            rows.append(self.n_users + h)\n",
    "            cols.append(self.n_users + t)\n",
    "\n",
    "        data = [1] * len(rows)\n",
    "        n_nodes = self.n_users + self.n_entities\n",
    "        adj = sp.coo_matrix((data, (rows, cols)), shape=(n_nodes, n_nodes))\n",
    "        adj = adj + adj.T.multiply(adj.T > 0) - adj.multiply(adj.T > 0)\n",
    "        norm_adj = self._normalize_adj(adj)\n",
    "        return self._convert_sp_mat_to_sp_tensor(norm_adj).to_dense(), {0: norm_adj}\n",
    "\n",
    "    def _normalize_adj(self, adj):\n",
    "        rowsum = np.array(adj.sum(1))\n",
    "        d_inv = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat = sp.diags(d_inv)\n",
    "        norm_adj = d_mat.dot(adj).dot(d_mat)\n",
    "        return norm_adj.tocoo()\n",
    "\n",
    "    def _convert_sp_mat_to_sp_tensor(self, mat):\n",
    "        mat = mat.tocoo()\n",
    "        indices = torch.from_numpy(np.vstack((mat.row, mat.col)).astype(np.int64))\n",
    "        values = torch.from_numpy(mat.data.astype(np.float32))\n",
    "        shape = torch.Size(mat.shape)\n",
    "        return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "    def generate_cf_batch(self, user_dict, batch_size):\n",
    "        users = list(user_dict.keys())\n",
    "        batch_users, batch_pos_items, batch_neg_items = [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            u = random.choice(users)\n",
    "            pos_items = user_dict[u]\n",
    "            if not pos_items:\n",
    "                continue\n",
    "            i = random.choice(pos_items)\n",
    "            j = random.randint(0, self.n_items - 1)\n",
    "            while j in pos_items:\n",
    "                j = random.randint(0, self.n_items - 1)\n",
    "            batch_users.append(u)\n",
    "            batch_pos_items.append(i)\n",
    "            batch_neg_items.append(j)\n",
    "        return torch.LongTensor(batch_users), torch.LongTensor(batch_pos_items), torch.LongTensor(batch_neg_items)\n",
    "\n",
    "    def generate_kg_batch(self, kg_dict, batch_size, entity_num):\n",
    "        h_list = list(kg_dict.keys())\n",
    "        batch_h, batch_r, batch_pos_t, batch_neg_t = [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            h = random.choice(h_list)\n",
    "            t_r_pairs = kg_dict[h]\n",
    "            if not t_r_pairs:\n",
    "                continue\n",
    "            t, r = random.choice(t_r_pairs)\n",
    "            neg_t = random.randint(0, entity_num - 1)\n",
    "            while neg_t in [x[0] for x in t_r_pairs]:\n",
    "                neg_t = random.randint(0, entity_num - 1)\n",
    "            batch_h.append(h)\n",
    "            batch_r.append(r)\n",
    "            batch_pos_t.append(t)\n",
    "            batch_neg_t.append(neg_t)\n",
    "        return torch.LongTensor(batch_h), torch.LongTensor(batch_r), torch.LongTensor(batch_pos_t), torch.LongTensor(batch_neg_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / Evaluation / Prediction functions\n",
    "\n",
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_batches = [user_ids[i:i+test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_batches = [torch.LongTensor(u) for u in user_batches]\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    all_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    for batch_users in tqdm(user_batches, desc=\"Evaluating\"):\n",
    "        batch_users = batch_users.to(device)\n",
    "        with torch.no_grad():\n",
    "            scores = model(batch_users, item_ids, mode='predict')\n",
    "        scores = scores.cpu()\n",
    "        batch_metrics = calc_metrics_at_k(scores, train_user_dict, test_user_dict, batch_users.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "        all_scores.append(scores.numpy())\n",
    "        for k in Ks:\n",
    "            for m in metric_names:\n",
    "                metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "\n",
    "    all_scores = np.concatenate(all_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return all_scores, metrics_dict\n",
    "\n",
    "def train(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    log_id = create_log_id(args.save_dir)\n",
    "    logging_config(args.save_dir, f'log{log_id}', no_console=False)\n",
    "    logging.info(args)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "    user_pre_embed = torch.tensor(data.user_pre_embed) if args.use_pretrain == 1 else None\n",
    "    item_pre_embed = torch.tensor(data.item_pre_embed) if args.use_pretrain == 1 else None\n",
    "\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    if args.use_pretrain == 2:\n",
    "        model = load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    cf_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kg_optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    best_epoch = -1\n",
    "    Ks = eval(args.Ks)\n",
    "    metric_store = {k: {m: [] for m in ['precision', 'recall', 'ndcg']} for k in Ks}\n",
    "    epoch_list = []\n",
    "\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        model.train()\n",
    "        cf_loss_total = 0\n",
    "        for _ in range(data.n_cf_train // args.cf_batch_size + 1):\n",
    "            u, pos_i, neg_i = data.generate_cf_batch(data.train_user_dict, args.cf_batch_size)\n",
    "            u, pos_i, neg_i = u.to(device), pos_i.to(device), neg_i.to(device)\n",
    "            cf_loss = model(u, pos_i, neg_i, mode='train_cf')\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_loss_total += cf_loss.item()\n",
    "\n",
    "        kg_loss_total = 0\n",
    "        for _ in range(data.n_kg_train // args.kg_batch_size + 1):\n",
    "            h, r, pt, nt = data.generate_kg_batch(data.train_kg_dict, args.kg_batch_size, data.n_users_entities)\n",
    "            h, r, pt, nt = h.to(device), r.to(device), pt.to(device), nt.to(device)\n",
    "            kg_loss = model(h, r, pt, nt, mode='train_kg')\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_loss_total += kg_loss.item()\n",
    "\n",
    "        h, t, r = data.h_list.to(device), data.t_list.to(device), data.r_list.to(device)\n",
    "        model(h, t, r, list(data.laplacian_dict.keys()), mode='update_att')\n",
    "\n",
    "        if epoch % args.evaluate_every == 0 or epoch == args.n_epoch:\n",
    "            scores, metrics = evaluate(model, data, Ks, device)\n",
    "            for k in Ks:\n",
    "                for m in ['precision', 'recall', 'ndcg']:\n",
    "                    metric_store[k][m].append(metrics[k][m])\n",
    "            epoch_list.append(epoch)\n",
    "            best_recall, stop = early_stopping(metric_store[min(Ks)]['recall'], args.stopping_steps)\n",
    "            if stop:\n",
    "                break\n",
    "            if metric_store[min(Ks)]['recall'][-1] == best_recall:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    records = {'epoch_idx': epoch_list}\n",
    "    for k in Ks:\n",
    "        for m in ['precision', 'recall', 'ndcg']:\n",
    "            records[f'{m}@{k}'] = metric_store[k][m]\n",
    "    pd.DataFrame(records).to_csv(os.path.join(args.save_dir, 'metrics.tsv'), sep='\\t', index=False)\n",
    "\n",
    "    # Print best\n",
    "    best_idx = epoch_list.index(best_epoch)\n",
    "    logging.info(f'Best @ Epoch {best_epoch}:')\n",
    "    for k in Ks:\n",
    "        for m in ['precision', 'recall', 'ndcg']:\n",
    "            logging.info(f'{m}@{k}: {metric_store[k][m][best_idx]:.4f}')\n",
    "\n",
    "def predict(args):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    model = load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "    Ks = eval(args.Ks)\n",
    "    scores, metrics = evaluate(model, data, Ks, device)\n",
    "    np.save(os.path.join(args.save_dir, 'cf_scores.npy'), scores)\n",
    "    for k in Ks:\n",
    "        print(f'Precision@{k}: {metrics[k][\"precision\"]:.4f}, Recall@{k}: {metrics[k][\"recall\"]:.4f}, NDCG@{k}: {metrics[k][\"ndcg\"]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Execution\n",
    "if __name__ == '__main__':\n",
    "    args = parse_kgat_args()\n",
    "    train(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
